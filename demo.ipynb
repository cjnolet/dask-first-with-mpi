{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import inotify\n",
    "except ImportError as e:\n",
    "    !pip install inotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import subprocess as subp\n",
    "import os\n",
    "import dask\n",
    "import inotify.adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38083\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>50.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:38083' processes=2 cores=2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(cluster)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hello_mpi import MPI_World\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_MPI_Demo:\n",
    "    \n",
    "    def __init__(self, client, uriFile = \"ompi.server.uri\" ):\n",
    "        self.client = client\n",
    "        self.uriFile = uriFile\n",
    "        \n",
    "        print(\"Starting ompi-server\")\n",
    "        self.create_ompi_server_()\n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        self.mpiServer.kill()\n",
    "        os.remove(self.uriFile)\n",
    "    \n",
    "    def create_ompi_server_(self):\n",
    "        \n",
    "        cmd = [\"ompi-server\", \"--no-daemonize\",\"-r\", self.uriFile]\n",
    "        cmdStr = \"exec \" + \" \".join(cmd)\n",
    "\n",
    "        i = inotify.adapters.Inotify()\n",
    "\n",
    "        with open(self.uriFile, 'w') as f:\n",
    "            pass\n",
    "\n",
    "        i.add_watch(self.uriFile)\n",
    "\n",
    "        proc = subp.Popen(cmdStr, shell=True)\n",
    "\n",
    "        # Polls the ompi-server uri file for \n",
    "        # changes. \n",
    "        # @todo: Make this robust to failures\n",
    "        for event in i.event_gen(yield_nones=False):\n",
    "            (_, type_names, path, filename) = event\n",
    "            if \"IN_CLOSE_WRITE\" in type_names:\n",
    "                break\n",
    "\n",
    "        i.remove_watch(self.uriFile)\n",
    "\n",
    "        mpiServer = proc\n",
    "        import time\n",
    "        with open(self.uriFile, \"r\") as fp:\n",
    "            mpiServerUri = fp.read().rstrip()\n",
    "\n",
    "        self.mpiServer = mpiServer\n",
    "        self.mpiServerUri = mpiServerUri\n",
    "\n",
    "        os.environ[\"OMPI_MCA_pmix_server_uri\"] = self.mpiServerUri\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_parse_host_port_(address):\n",
    "        if '://' in address:\n",
    "            address = address.rsplit('://', 1)[1]\n",
    "        host, port = address.split(':')\n",
    "        port = int(port)\n",
    "        return host, port\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, ompiServerUri):\n",
    "        if ompiServerUri is None:\n",
    "            raise Exception(\"ompiServerUri is mandatory!\")\n",
    "        os.environ[\"OMPI_MCA_pmix_server_uri\"] = ompiServerUri\n",
    "        w = dask.distributed.get_worker()\n",
    "        print(\"Hello World! from ip=%s worker=%s/%d uri=%s\" % \\\n",
    "              (w.address, w.name, nWorkers, ompiServerUri))\n",
    "        print(\"Worker=%s finished\" % w.name)\n",
    "\n",
    "        a = MPI_World(workerId, nWorkers)\n",
    "        a.init()\n",
    "        a.create_builder()\n",
    "\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_build_session_(world, r):\n",
    "        world.new_session()\n",
    "        \n",
    "    @staticmethod\n",
    "    def func_open_server_port_(world, r):\n",
    "        world.open_server_port()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def func_get_server_port_(world, r):\n",
    "        world.get_server_port()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_connect_to_server_(world, r):\n",
    "        world.connect_to_server()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_connect_to_client_(world, clientId, r):\n",
    "        world.connect_to_client(clientId)\n",
    "\n",
    "    @staticmethod\n",
    "    def func_merge_clients_(world, r):\n",
    "        world.merge_clients()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def func_get_rank_(world, r):\n",
    "        return world.rank()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_finalize_(world, r):\n",
    "        world.finalize()\n",
    "        \n",
    "        \n",
    "    def get_workers_(self):\n",
    "        return list(map(lambda x: Dask_MPI_Demo.func_parse_host_port_(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init(self):\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        \n",
    "        w, i = workers_indices[0]\n",
    "        self.server = self.client.submit(Dask_MPI_Demo.func_init_,\n",
    "                                               i, len(workers), self.mpiServerUri, workers=[w])\n",
    "        \n",
    "        \n",
    "        self.clients = [(idx, worker, self.client.submit(Dask_MPI_Demo.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           self.mpiServerUri, \n",
    "                                           workers=[worker])) \n",
    "             for worker, idx in workers_indices[1:]]\n",
    "                       \n",
    "        \n",
    "    def build_session(self):\n",
    "        \n",
    "        print(\"Building server\")\n",
    "                       \n",
    "        self.client.submit(Dask_MPI_Demo.func_open_server_port_, self.server, random.random()).result()\n",
    "        [self.client.submit(Dask_MPI_Demo.func_get_server_port_, f, random.random()).result() for i, w, f in self.clients]\n",
    "\n",
    "        print(\"Connecting clients to server\")\n",
    "        for idx, worker, cur_client in self.clients:\n",
    "            s = self.client.submit(Dask_MPI_Demo.func_connect_to_server_, cur_client, random.random())\n",
    "            c = self.client.submit(Dask_MPI_Demo.func_connect_to_client_, self.server, idx, random.random())\n",
    "            \n",
    "            wait([s, c])\n",
    "            \n",
    "        print(\"Merging client ranks\")\n",
    "        for idx, worker, cur_client in self.clients:\n",
    "            self.client.submit(Dask_MPI_Demo.func_merge_clients_, cur_client, random.random()).result()\n",
    "        \n",
    "    def finalize(self):\n",
    "        [c.submit(Dask_MPI_Demo.func_finalize_, a, random.random()) for i, w, a in self.clients]\n",
    "        c.submit(Dask_MPI_Demo.func_finalize_, self.server, random.random())\n",
    "        self.clients = None\n",
    "        self.server = None\n",
    "        \n",
    "    def get_client_ranks(self):\n",
    "        return [c.submit(Dask_MPI_Demo.func_get_rank_, a, random.random()).result() for i, w, a in self.clients]\n",
    "    \n",
    "    def get_server_rank(self):\n",
    "        return c.submit(Dask_MPI_Demo.func_get_rank_, self.server, random.random()).result()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ompi-server\n"
     ]
    }
   ],
   "source": [
    "demo = Dask_MPI_Demo(c)\n",
    "demo.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building server\n",
      "Connecting clients to server\n",
      "Merging client ranks\n"
     ]
    }
   ],
   "source": [
    "demo.build_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Ranks: [1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Client Ranks: \" + str(demo.get_client_ranks()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server Rank: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Server Rank: \" + str(demo.get_server_rank()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
